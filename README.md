_Здесь написаны основные идеи, заметки при обучении на 3 курсе._  
_Формулы корректно отображаются на [этом сайте](https://upmath.me/)_

#### 27/09/2020 - повторил 2 первые главы [курса](https://stepik.org/course/54098/promo)
##### Глава 1
- Генерация текста:
   - через поиск похожих
   - по шаблону
   - с помощью нейросетей
- При векторном разряженном представлении документа теряется зависимость слов
- Предиктивные модели (BERT, Transformer и т.п.) не требуют размеченной выборки
- Сходство текстов можно определить как долю совпадающих путей, проходимых в графовых представлениях текстов   
- В классификации с текстами:
   - большой длины линейные модели дают основное качество
   - короткими, в зависимости от объема gold_labels:
     - малый объем - ядерные методы
     - совсем нет - системы правил

- В эксплоративном анализе применяются методы тематического регулирования: LDA, ARTM

##### Глава 2
- В подходе с TF-IDF не используется информация о метках документов => теряем часть информация, если она есть


#### 04/10/2020 - прочитал обзорную [статью](https://arxiv.org/pdf/1802.02871.pdf) про online learning
- В большенстве случаев рассматривается бинарная классификация и задача оптимизации $$ R_{T}=\sum_{t=1}^{T} \ell_{t}\left(\mathbf{w}_{t}\right)-\min _{\mathbf{w}} \sum_{t=1}^{T} \ell_{t}(\mathbf{w}) $$, причём $$min$$ ищется для $$w$$, не зависящего от $$t$$
- В Contextual Bandits минимизируется $$ R_{T}(f)=\sum_{t=1}^{T}\left[\ell_{I_{t}, t}-\ell_{t}\left(f^{*}\right)\right] $$, где $$ f^{*}=\arg \inf _{f \in \mathcal{F}} \ell_{D}(f) $$, &nbsp; $$ I_{t} \in \overline{1, k} $$ - действие выбранное на t шаге
- Есть ссылка на потенциально интересную [статью](https://arxiv.org/pdf/1711.03705.pdf) про online deep learning
